{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# https://github.com/udacity/deep-learning/blob/master/seq2seq/sequence_to_sequence_implementation.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "from os import listdir\n",
    "import re\n",
    "import pandas as pd\n",
    "import glob\n",
    "import unicodedata\n",
    "import string\n",
    "\n",
    "import copy\n",
    "import pickle\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_000.bin\n",
      "train_001.bin\n",
      "train_002.bin\n",
      "train_003.bin\n",
      "train_004.bin\n",
      "train_005.bin\n",
      "train_006.bin\n",
      "train_007.bin\n",
      "train_008.bin\n",
      "train_009.bin\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "path = \"./data/finished_files/chunked/\" # test_000.bin\"\n",
    "train_raw_all = []\n",
    "\n",
    "for filename in listdir(path):\n",
    "    if 'train_00' in filename:\n",
    "        print(filename)\n",
    "        path_file = path + '\\\\' + filename\n",
    "        with open(path_file, 'r', encoding='utf-8',errors='ignore') as binary_file:\n",
    "            raw_text = binary_file.read()\n",
    "            #raw_text = raw_text.replace('<s>', '')\n",
    "            #raw_text = raw_text.replace('</s>', '')\n",
    "            raw_text = raw_text.replace('\\n', '')\n",
    "            raw_text = raw_text.replace('\\x12', '')\n",
    "            raw_text = raw_text.replace('\\x17', '')\n",
    "            raw_text = raw_text.replace('\\x01', '')\n",
    "            raw_text = raw_text.replace('\\x02', '')\n",
    "            raw_text = raw_text.replace('\\x03', '')\n",
    "            raw_text = raw_text.replace('\\x00', '')\n",
    "            raw_text = raw_text.replace('\\x11', '')\n",
    "            raw_text = raw_text.replace('\\x14', '')\n",
    "            raw_text = raw_text.replace('\\x15', '')\n",
    "            abstract = raw_text.split('\\x08abstract') # .decode('utf-8', 'ignore')ISO-8859-1 \n",
    "            x = [a.split('\\x07article') for a in abstract]\n",
    "            train_raw_all.extend(x[1:])\n",
    "   \n",
    "        \n",
    "# df = pd.DataFrame(data=train_raw_all, columns=['abstract','article']) # df = df.ix[1:]\n",
    "# s = {'<s>':'', '</s>':''}\n",
    "# df['abstract'] = df['abstract'].replace(s)\n",
    "\n",
    "#df['abstract'] = df['abstract'].apply(lambda x: x.split('<s>',1)[-1])\n",
    "#df['article'] = df['article'].apply(lambda x: x.split('-rrb-',1)[-1])\n",
    "\n",
    "print('done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"<s> lubos michel will referee wednesday 's champions league final in moscow . </s> <s> fellow-slovakians roman slysko and martin balko will be his two assistants . </s> <s> vladimir hrinak will be fourth official at the manchester utd v chelsea clash . </s>\",\n",
       " \"moscow , russia -- uefa have confirmed that slovakian referee lubos michel will take charge of wednesday 's champions league final between manchester united and chelsea in moscow . michel refereed the 2003 uefa cup final when jose mourinho 's porto beat celtic . michel , 40 last week , is remembered by chelsea fans for controversially awarding a goal to liverpool against the londoners in the semifinals of the 2005 competition . chelsea 's manager at the time jose mourinho always insisted that luis garcia 's shot did not cross the line . michel is regarded as one of the top referees in the world and officiated at euro 2004 and the 2006 world cup , including the tense germany v argentina game in the first knock-out round . he also refereed the 2003 uefa cup final when mourinho 's porto triumphed over celtic . michel speaks english , russian , german and polish as well as his native language . michel will be supported at the luzhniki stadium by the assistant referees who have partnered him at major tournaments - roman slysko -lrb- 34 -rrb- and martin balko -lrb- 36 -rrb- . the fourth official will be vladimir hrinak -lrb- 44 -rrb- , also from slovakia . meanwhile , senior police officers from manchester traveled to moscow on monday to help prevent fan violence from marring the final . manchester police will serve in an advisory role as russian authorities prepare for wednesday 's match . `` a number of my officers and i have flown out to moscow to meet local police and help draw up arrangements to police the fixture , '' said chief superintendent janette mccormick . `` although gmp officers have no powers of arrest abroad and ultimate responsibility for policing the game lies with the russian authorities , we have been gathering intelligence on potential and known troublemakers and gmp officers will be in moscow in an advisory role , '' mccormick added . as manchester united flew out on monday , plain-clothed intelligence officers were stationed at manchester airport to target known or potential troublemakers from heading to the russian capital . ban orders imposed on fans with soccer-related convictions will be rigorously enforced in the next two days to prevent them from flying to moscow . `` as with all operations like this , we are sending out a clear message to people intending to travel to commit violence to think again , '' said police chief inspector robert tinsley , who is based at the airport . the airport is expecting 20,000 more passengers than usual to be flying to moscow . the british embassy in moscow have announced that their consular section will stay open on may 21 and 22 to help english fans arriving for the final . `` the consular department will extend their working hours on wednesday and thursday to support english citizens coming here to watch the champions ' league final , '' the embassy press service said . the press service report added that a 24-hour telephone `` hot line '' would be organized by the embassy .\\x08\"]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(train_raw_all[737])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<s> mentally ill inmates in miami are housed on the `` forgotten floor '' </s> <s> judge steven leifman says most are there as a result of `` avoidable felonies '' </s> <s> while cnn tours facility , patient shouts : `` i am the son of the president '' </s> <s> leifman says the system is unjust and he 's fighting for change . </s>  <END> <s> harry potter star daniel radcliffe gets # 20m fortune as he turns 18 monday . </s> <s> young actor says he has no plans to fritter his cash away . </s> <s> radcliffe 's earnings from first five potter films have been held in trust fund . </s> <END> <s> new : `` i thought i was going to die , '' driver says . </s> <s> man says pickup truck was folded in half ; he just has cut on face . </s> <s> driver : `` i probably had a 30 - , 35-foot free fall '' </s> <s> minnesota bridge collapsed during rush hour wednesday . </s>  <END> <s> parents beam with pride , ca n't stop from smiling from outpouring of support . </s> <s> mom : `` i was so happy i did n't know what to do '' </s> <s> burn center in u.s. has offered to provide treatment for reconstructive surgeries\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_text = (' <END> ').join([a[1] for a in train_raw_all])\n",
    "target_text = (' <END> ').join([a[0] for a in train_raw_all])\n",
    "\n",
    "(target_text[:1111])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# df = df[['abstract', 'article']]\n",
    "# df['abstract'] = df['abstract'].apply(lambda x: x.strip(' '))\n",
    "\n",
    "# source_text = (df['article'])\n",
    "# target_text = str(df['abstract'].apply(lambda x: str(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total no. of abstracts/articles: 10000\n"
     ]
    }
   ],
   "source": [
    "print('Total no. of abstracts/articles: {}'.format(len(source_text.split('<END>'))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approximate number of unique words in all articles: 103110\n",
      "Approximate number of unique words in all abtracts: 30121\n"
     ]
    }
   ],
   "source": [
    "print('Approximate number of unique words in all articles: {}'.format(len({word:None for word in str(source_text).split()})))\n",
    "print('Approximate number of unique words in all abtracts: {}'.format(len({word:None for word in str(target_text).split()})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of words in an article: 720.847\n",
      "Average number of words in an abstract: 55.8814\n"
     ]
    }
   ],
   "source": [
    "arti = source_text.split(' <END> ')\n",
    "abst = target_text.split(' <END> ')\n",
    "\n",
    "word_counts1 = [len(a.split()) for a in arti]\n",
    "word_counts2 = [len(a.split()) for a in abst]\n",
    "\n",
    "print('Average number of words in an article: {}'.format(np.average(word_counts1)))\n",
    "print('Average number of words in an abstract: {}'.format(np.average(word_counts2)))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# OLD\n",
    "\n",
    "CODES = {'<PAD>': 0, ' <END> ': 1, '<UNK>': 2, '<GO>': 3} # \n",
    "\n",
    "def create_lookup_tables(text):\n",
    "    \"\"\"\n",
    "    Create lookup tables for vocabulary\n",
    "    \"\"\"\n",
    "    vocab = set(text.split()) \n",
    "    vocab_to_int = copy.copy(CODES)\n",
    "\n",
    "    for v_i, v in enumerate(vocab, len(CODES)):\n",
    "        vocab_to_int[v] = v_i\n",
    "\n",
    "    int_to_vocab = {v_i:v for v, v_i in vocab_to_int.items()}\n",
    "\n",
    "    return vocab_to_int, int_to_vocab\n",
    "\n",
    "\n",
    "def text_to_ids(source_text, target_text, source_vocab_to_int, target_vocab_to_int):\n",
    "    \"\"\"\n",
    "    Convert source and target text to proper word ids\n",
    "    :param source_text: String that contains all the source text.\n",
    "    :param target_text: String that contains all the target text.\n",
    "    :param source_vocab_to_int: Dictionary to go from the source words to an id\n",
    "    :param target_vocab_to_int: Dictionary to go from the target words to an id\n",
    "    :return: A tuple of lists (source_id_text, target_id_text)\n",
    "    \"\"\"\n",
    "    \n",
    "    source_id_text = [[source_vocab_to_int[w] for w in arti.split()] for arti in source_text.split(' <END> ')] #.split('\\n')]\n",
    "    \n",
    "    target_id_text = [[target_vocab_to_int[w] for w in abst.split()]+[target_vocab_to_int[' <END> ']] for abst in target_text.split(' <END> ')]\n",
    "\n",
    "    return source_id_text, target_id_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# new \n",
    "def extract_character_vocab(data):\n",
    "    special_words = ['<PAD>', '<UNK>', '<GO>',  ' <END> ']\n",
    "\n",
    "    # set_words = set([word for line in data.split() for word in line])\n",
    "    set_words = set(data.split())\n",
    "    int_to_vocab = {word_i: word for word_i, word in enumerate(special_words + list(set_words))}\n",
    "    vocab_to_int = {word: word_i for word_i, word in int_to_vocab.items()}\n",
    "\n",
    "    return int_to_vocab, vocab_to_int\n",
    "\n",
    "\n",
    "# Build int2vocab and vocab2int dicts\n",
    "source_int_to_vocab, source_vocab_to_int = extract_character_vocab(source_text)\n",
    "target_int_to_vocab, target_vocab_to_int = extract_character_vocab(target_text)\n",
    "\n",
    "\n",
    "# Convert characters to ids\n",
    "source_vocab_ids = [[source_vocab_to_int.get(word, source_vocab_to_int['<UNK>']) for word in article]\\\n",
    "                     for article in source_text.split(' <END> ')]\n",
    "\n",
    "target_vocab_ids = [[target_vocab_to_int.get(word, target_vocab_to_int['<UNK>']) for word in abstract]\\\n",
    "                     + [target_vocab_to_int[' <END> ']] for abstract in target_text.split(' <END> ')] \n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# preprocess & save data \n",
    "\n",
    "# source_vocab_to_int, source_int_to_vocab = create_lookup_tables(source_text)\n",
    "# target_vocab_to_int, target_int_to_vocab = create_lookup_tables(target_text)\n",
    "\n",
    "# source_text_, target_text_ = text_to_ids(source_text, target_text, source_vocab_to_int, target_vocab_to_int)\n",
    "\n",
    "with open('preprocess.p', 'wb') as out_file:\n",
    "    pickle.dump((\n",
    "        (source_vocab_ids, target_vocab_ids),\n",
    "        (source_vocab_to_int, target_vocab_to_int),\n",
    "        (source_int_to_vocab, target_int_to_vocab)), out_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CHECKPOINT 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.3.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jermyn Bek\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:15: UserWarning: No GPU found. Please use a GPU to train your neural network.\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "CHECK TENSORFLOW & GPU\n",
    "\"\"\"\n",
    "from distutils.version import LooseVersion\n",
    "import warnings\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.layers.core import Dense\n",
    "\n",
    "# Check TensorFlow Version\n",
    "# assert LooseVersion(tf.__version__) in [LooseVersion('1.0.0'), LooseVersion('1.0.1')], 'This project requires TensorFlow version 1.0  You are using {}'.format(tf.__version__)\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "\n",
    "# Check for a GPU\n",
    "if not tf.test.gpu_device_name():\n",
    "    warnings.warn('No GPU found. Please use a GPU to train your neural network.')\n",
    "else:\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import problem_unittests\n",
    "\n",
    "def load_preprocess():\n",
    "    \"\"\"\n",
    "    Load the Preprocessed Training data and return them in batches of <batch_size> or less\n",
    "    \"\"\"\n",
    "    with open('preprocess.p', mode='rb') as in_file:\n",
    "        return pickle.load(in_file)\n",
    "\n",
    "\n",
    "(source_vocab_ids, target_vocab_ids), (source_vocab_to_int, target_vocab_to_int), (source_int_to_vocab, target_int_to_vocab) = load_preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "103114"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(source_vocab_to_int)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "print(\"Example source sequence\")\n",
    "print(source_vocab_ids[:1])\n",
    "print(\"\\n\")\n",
    "print(\"Example target sequence\")\n",
    "print(target_vocab_ids[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Number of Epochs\n",
    "epochs = 60\n",
    "\n",
    "# Batch Size\n",
    "batch_size = 128\n",
    "\n",
    "# RNN Size\n",
    "rnn_size = 50\n",
    "\n",
    "# Number of Layers\n",
    "num_layers = 2\n",
    "\n",
    "# Embedding Size\n",
    "encoding_embedding_size = 155\n",
    "decoding_embedding_size = 155\n",
    "\n",
    "# Learning Rate\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model_inputs()\n",
    "To create TF Placeholders for the Neural Network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "def model_inputs():\n",
    "    inputs = tf.placeholder(tf.int32, [None,None], name='input')\n",
    "    targets = tf.placeholder(tf.int32, [None,None], name='target')\n",
    "    learning_rates = tf.placeholder(tf.float32)\n",
    "    keep_probs = tf.placeholder(tf.float32, name='keep_prob')\n",
    "    \n",
    "    return inputs, targets, learning_rates, keep_probs\n",
    "\"\"\"\n",
    "# new\n",
    "def get_model_inputs():\n",
    "    input_data = tf.placeholder(tf.int32, [None, None], name='input')\n",
    "    targets = tf.placeholder(tf.int32, [None, None], name='targets')\n",
    "    lr = tf.placeholder(tf.float32, name='learning_rate')\n",
    "\n",
    "    target_sequence_length = tf.placeholder(tf.int32, (None,), name='target_sequence_length')\n",
    "    max_target_sequence_length = tf.reduce_max(target_sequence_length, name='max_target_len')\n",
    "    source_sequence_length = tf.placeholder(tf.int32, (None,), name='source_sequence_length')\n",
    "    \n",
    "    return input_data, targets, lr, target_sequence_length, max_target_sequence_length, source_sequence_length\n",
    "\n",
    "# problem_unittests.test_model_inputs(model_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### encoding_layer() \n",
    "To create an encoder RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "def encoding_layer(rnn_inputs, rnn_size, num_layers, keep_prob):\n",
    "    # tf v1.0\n",
    "    # LSTM = tf.contrib.rnn.BasicLSTMCell(rnn_size)\n",
    "    # enc_cell = tf.contrib.rnn.MultiRNNCell([LSTM] * num_layers)\n",
    "    \n",
    "    def lstm_cell():\n",
    "        return tf.nn.rnn_cell.BasicLSTMCell(rnn_size)\n",
    "    \n",
    "    stacked_lstm = tf.nn.rnn_cell.MultiRNNCell([lstm_cell() for _ in range(num_layers)])\n",
    "    stacked_lstm2 = tf.nn.rnn_cell.DropoutWrapper(stacked_lstm, output_keep_prob=keep_prob)\n",
    "    enc_output, enc_state = tf.nn.dynamic_rnn(stacked_lstm2, rnn_inputs, dtype=tf.float32)\n",
    "    \n",
    "    return enc_state\n",
    "\"\"\"\n",
    "\n",
    "### new\n",
    "def encoding_layer(input_data, rnn_size, num_layers, source_sequence_length, \n",
    "                   source_vocab_size, encoding_embedding_size):\n",
    "    \n",
    "    # Encoder embedding\n",
    "    enc_embed_input = tf.contrib.layers.embed_sequence(input_data, source_vocab_size, encoding_embedding_size)\n",
    "\n",
    "    # RNN cell\n",
    "    def make_cell(rnn_size):\n",
    "        enc_cell = tf.contrib.rnn.LSTMCell(rnn_size,\n",
    "                                           initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2))\n",
    "        return enc_cell\n",
    "\n",
    "    enc_cell = tf.contrib.rnn.MultiRNNCell([make_cell(rnn_size) for _ in range(num_layers)])\n",
    "    \n",
    "    enc_output, enc_state = tf.nn.dynamic_rnn(enc_cell, enc_embed_input, sequence_length=source_sequence_length, dtype=tf.float32)\n",
    "    \n",
    "    return enc_output, enc_state\n",
    "\n",
    "\n",
    "\n",
    "# DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\n",
    "# problem_unittests.test_encoding_layer(encoding_layer) \n",
    "# pip install dask --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### process_decoding_input() \n",
    "\n",
    "To remove the last word id from each batch in target_data and concat the GO ID to the beginning of each batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "def process_decoding_input(target_data, target_vocab_to_int, batch_size):\n",
    "    ending = tf.strided_slice(target_data, begin=[0,0], end=[batch_size,-1], strides=[1,1])\n",
    "    decoder_input = tf.concat([tf.fill(dims=[batch_size,1], value=target_vocab_to_int['<GO>']), ending], axis=1)\n",
    "    \n",
    "    return decoder_input\n",
    "\"\"\"\n",
    "\n",
    "# new\n",
    "def process_decoder_input(target_data, target_vocab_to_int, batch_size):\n",
    "    ending = tf.strided_slice(target_data, [0, 0], [batch_size, -1], [1, 1])\n",
    "    dec_input = tf.concat([tf.fill([batch_size, 1], target_vocab_to_int['<GO>']), ending], 1)\n",
    "\n",
    "    return dec_input\n",
    "\n",
    "# problem_unittests.test_process_decoding_input(process_decoding_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### decoding_layer() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def decoding_layer(target_vocab_to_int, decoding_embedding_size, num_layers, rnn_size,\n",
    "                   target_sequence_length, max_target_sequence_length, enc_state, dec_input):\n",
    "    \n",
    "    # 1. Decoder Embedding\n",
    "    target_vocab_size = len(target_vocab_to_int)\n",
    "    dec_embeddings = tf.Variable(tf.random_uniform([target_vocab_size, decoding_embedding_size]))\n",
    "    dec_embed_input = tf.nn.embedding_lookup(params=dec_embeddings, ids=dec_input)\n",
    "\n",
    "    # 2. Construct the decoder cell\n",
    "    def make_cell(rnn_size):\n",
    "        dec_cell = tf.contrib.rnn.LSTMCell(rnn_size, \n",
    "                                           initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2))\n",
    "        return dec_cell\n",
    "\n",
    "    dec_cell = tf.contrib.rnn.MultiRNNCell([make_cell(rnn_size) for _ in range(num_layers)])\n",
    "     \n",
    "    # 3. Dense layer to translate the decoder's output at each time \n",
    "    # step into a choice from the target vocabulary\n",
    "    output_layer = Dense(target_vocab_size,\n",
    "                         kernel_initializer = tf.truncated_normal_initializer(mean = 0.0, stddev=0.1))\n",
    "    \n",
    "    # 4. Set up a training decoder and an inference decoder\n",
    "    # Training Decoder\n",
    "    with tf.variable_scope(\"decode\"):\n",
    "\n",
    "        # Helper for the training process. Used by BasicDecoder to read inputs.\n",
    "        training_helper = tf.contrib.seq2seq.TrainingHelper(inputs=dec_embed_input,\n",
    "                                                            sequence_length=target_sequence_length,\n",
    "                                                            time_major=False)\n",
    "        \n",
    "        \n",
    "        # Basic decoder\n",
    "        training_decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell, training_helper,\n",
    "                                                           enc_state, output_layer) \n",
    "        \n",
    "        # Perform dynamic decoding using the decoder\n",
    "        training_decoder_output = tf.contrib.seq2seq.dynamic_decode(training_decoder, impute_finished=True, \n",
    "                                                                    maximum_iterations=max_target_sequence_length)[0]\n",
    "        \n",
    "    # 5. Inference Decoder\n",
    "    # Reuses the same parameters trained by the training process\n",
    "    with tf.variable_scope(\"decode\", reuse=True):\n",
    "        start_tokens = tf.tile(tf.constant([target_vocab_to_int['<GO>']], dtype=tf.int32), [batch_size], name='start_tokens')\n",
    "\n",
    "        # Helper for the inference process.\n",
    "        inference_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(dec_embeddings, \n",
    "                                                                    start_tokens, \n",
    "                                                                    target_vocab_to_int[' <END> '])\n",
    "\n",
    "        # Basic decoder\n",
    "        inference_decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell, inference_helper, enc_state, output_layer)\n",
    "        \n",
    "        # Perform dynamic decoding using the decoder\n",
    "        inference_decoder_output = tf.contrib.seq2seq.dynamic_decode(inference_decoder, impute_finished=True, \n",
    "                                                                     maximum_iterations=max_target_sequence_length)[0]\n",
    "    \n",
    "    return training_decoder_output, inference_decoder_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seq2seq model\n",
    "\n",
    "merge encoder with decoder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def seq2seq_model(input_data, targets, lr, target_sequence_length, \n",
    "                  max_target_sequence_length, source_sequence_length,\n",
    "                  source_vocab_size, target_vocab_size,\n",
    "                  encoding_embedding_size, decoding_embedding_size, \n",
    "                  rnn_size, num_layers):\n",
    "    \n",
    "    # Pass the input data through the ENCODER. We'll ignore the encoder output, but use the state\n",
    "    _, enc_state = encoding_layer(input_data, \n",
    "                                  rnn_size, \n",
    "                                  num_layers, \n",
    "                                  source_sequence_length,\n",
    "                                  source_vocab_size, \n",
    "                                  encoding_embedding_size)\n",
    "    \n",
    "    \n",
    "    # PREPARE the target sequences we'll feed to the decoder in training mode\n",
    "    dec_input = process_decoder_input(targets, target_vocab_to_int, batch_size)\n",
    "    \n",
    "    # Pass encoder state and decoder inputs to the decoders\n",
    "    training_decoder_output, inference_decoder_output = decoding_layer(target_vocab_to_int, \n",
    "                                                                       decoding_embedding_size, \n",
    "                                                                       num_layers, \n",
    "                                                                       rnn_size,\n",
    "                                                                       target_sequence_length,\n",
    "                                                                       max_target_sequence_length,\n",
    "                                                                       enc_state, \n",
    "                                                                       dec_input) \n",
    "    \n",
    "    return training_decoder_output, inference_decoder_output\n",
    "\n",
    "# note: both contain a 'rnn_output' logits tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# build the graph\n",
    "\n",
    "train_graph = tf.Graph()\n",
    "# Set the graph to default to ensure that it is ready for training\n",
    "with train_graph.as_default():\n",
    "    \n",
    "    # Load the model inputs    \n",
    "    input_data, targets, lr, target_sequence_length, max_target_sequence_length, source_sequence_length = get_model_inputs()\n",
    "    \n",
    "    # Create the training and inference logits\n",
    "    training_decoder_output, inference_decoder_output = seq2seq_model(input_data, \n",
    "                                                                      targets, \n",
    "                                                                      lr, \n",
    "                                                                      target_sequence_length, \n",
    "                                                                      max_target_sequence_length, \n",
    "                                                                      source_sequence_length,\n",
    "                                                                      len(source_vocab_to_int),\n",
    "                                                                      len(target_vocab_to_int),\n",
    "                                                                      encoding_embedding_size, \n",
    "                                                                      decoding_embedding_size, \n",
    "                                                                      rnn_size, \n",
    "                                                                      num_layers)    \n",
    "    \n",
    "    # Create tensors for the training logits and inference logits\n",
    "    training_logits = tf.identity(training_decoder_output.rnn_output, 'logits') # ?\n",
    "    inference_logits = tf.identity(inference_decoder_output.sample_id, name='predictions') # ?\n",
    "    \n",
    "    # Create the weights for sequence_loss\n",
    "    masks = tf.sequence_mask(target_sequence_length, max_target_sequence_length, dtype=tf.float32, name='masks')\n",
    "\n",
    "    with tf.name_scope(\"optimization\"):\n",
    "        \n",
    "        # Loss function\n",
    "        cost = tf.contrib.seq2seq.sequence_loss(training_logits, targets, masks)\n",
    "\n",
    "        # Optimizer\n",
    "        optimizer = tf.train.AdamOptimizer(lr)\n",
    "\n",
    "        # Gradient Clipping\n",
    "        gradients = optimizer.compute_gradients(cost)\n",
    "        capped_gradients = [(tf.clip_by_value(grad, -5., 5.), var) for grad, var in gradients if grad is not None]\n",
    "        train_op = optimizer.apply_gradients(capped_gradients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Get batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pad articles/abstracts with <PAD> so that each articles/abstracts of a batch has the same length\n",
    "\n",
    "def pad_batch(article_batch, pad_int):\n",
    "    max_article = max([len(a) for a in article_batch])\n",
    "    \n",
    "    return [a + [pad_int] * (max_article - len(a)) for a in article_batch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Batch targets, sources, and the lengths of their articles/abstracts together\n",
    "\n",
    "def get_batches(targets, sources, batch_size, source_pad_int, target_pad_int):\n",
    "    for batch_i in range(0, len(sources)//batch_size):\n",
    "        start_i = batch_i * batch_size\n",
    "        sources_batch = sources[start_i:start_i + batch_size]\n",
    "        targets_batch = targets[start_i:start_i + batch_size]\n",
    "        pad_sources_batch = np.array(pad_batch(sources_batch, source_pad_int))\n",
    "        pad_targets_batch = np.array(pad_batch(targets_batch, target_pad_int))\n",
    "        \n",
    "        # Need the lengths for the _lengths parameters\n",
    "        pad_targets_lengths = []\n",
    "        for target in pad_targets_batch:\n",
    "            pad_targets_lengths.append(len(target))\n",
    "        \n",
    "        pad_source_lengths = []\n",
    "        for source in pad_sources_batch:\n",
    "            pad_source_lengths.append(len(source))\n",
    "        \n",
    "        yield pad_targets_batch, pad_sources_batch, pad_targets_lengths, pad_source_lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split data to training and validation sets\n",
    "train_source = source_vocab_ids[batch_size:]\n",
    "train_target = target_vocab_ids[batch_size:]\n",
    "valid_source = source_vocab_ids[:batch_size]\n",
    "valid_target = target_vocab_ids[:batch_size]\n",
    "\n",
    "(valid_targets_batch, valid_sources_batch, \n",
    " valid_targets_lengths, valid_sources_lengths) = next(get_batches(valid_target,\n",
    "                                                                  valid_source,\n",
    "                                                                  batch_size,\n",
    "                                                                  source_vocab_to_int['<PAD>'],\n",
    "                                                                  target_vocab_to_int['<PAD>']))\n",
    "\n",
    "display_step = 2 # Check training loss after every 20 batches\n",
    "\n",
    "checkpoint = \"best_model.ckpt\" \n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "    for epoch_i in range(1, epochs+1):\n",
    "        for batch_i, (targets_batch, sources_batch, targets_lengths, sources_lengths) in\\\n",
    "        enumerate(get_batches(train_target, train_source, batch_size,\n",
    "                              source_vocab_to_int['<PAD>'],target_vocab_to_int['<PAD>'])):\n",
    "            \n",
    "            # Training step\n",
    "            _, loss = sess.run(\n",
    "                [train_op, cost],\n",
    "                {input_data: sources_batch,\n",
    "                 targets: targets_batch,\n",
    "                 lr: learning_rate,\n",
    "                 target_sequence_length: targets_lengths,\n",
    "                 source_sequence_length: sources_lengths})\n",
    "\n",
    "            # Debug message updating us on the status of the training\n",
    "            if batch_i % display_step == 0 and batch_i > 0:\n",
    "                \n",
    "                # Calculate validation cost\n",
    "                validation_loss = sess.run(\n",
    "                [cost],\n",
    "                {input_data: valid_sources_batch,\n",
    "                 targets: valid_targets_batch,\n",
    "                 lr: learning_rate,\n",
    "                 target_sequence_length: valid_targets_lengths,\n",
    "                 source_sequence_length: valid_sources_lengths})\n",
    "                \n",
    "                print('Epoch {:>3}/{} Batch {:>4}/{} - Loss: {:>6.3f}  - Validation loss: {:>6.3f}'\n",
    "                      .format(epoch_i,\n",
    "                              epochs, \n",
    "                              batch_i, \n",
    "                              len(train_source) // batch_size, \n",
    "                              loss, \n",
    "                              validation_loss[0]))\n",
    "\n",
    "    \n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess, checkpoint)\n",
    "    print('Model Trained and Saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def source_to_seq(text):\n",
    "    '''Prepare the text for the model'''\n",
    "    sequence_length = 7\n",
    "    return [source_letter_to_int.get(word, source_letter_to_int['<UNK>']) for word in text]+ [source_letter_to_int['<PAD>']]*(sequence_length-len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_sentence = 'hello'\n",
    "text = source_to_seq(input_sentence)\n",
    "\n",
    "checkpoint = \"./best_model.ckpt\"\n",
    "\n",
    "loaded_graph = tf.Graph()\n",
    "with tf.Session(graph=loaded_graph) as sess:\n",
    "    # Load saved model\n",
    "    loader = tf.train.import_meta_graph(checkpoint + '.meta')\n",
    "    loader.restore(sess, checkpoint)\n",
    "\n",
    "    input_data = loaded_graph.get_tensor_by_name('input:0')\n",
    "    logits = loaded_graph.get_tensor_by_name('predictions:0')\n",
    "    source_sequence_length = loaded_graph.get_tensor_by_name('source_sequence_length:0')\n",
    "    target_sequence_length = loaded_graph.get_tensor_by_name('target_sequence_length:0')\n",
    "    \n",
    "    #Multiply by batch_size to match the model's input parameters\n",
    "    answer_logits = sess.run(logits, {input_data: [text]*batch_size, \n",
    "                                      target_sequence_length: [len(text)]*batch_size, \n",
    "                                      source_sequence_length: [len(text)]*batch_size})[0] \n",
    "\n",
    "\n",
    "pad = source_letter_to_int[\"<PAD>\"] \n",
    "\n",
    "print('Original Text:', input_sentence)\n",
    "\n",
    "print('\\nSource')\n",
    "print('  Word Ids:    {}'.format([i for i in text]))\n",
    "print('  Input Words: {}'.format(\" \".join([source_int_to_letter[i] for i in text])))\n",
    "\n",
    "print('\\nTarget')\n",
    "print('  Word Ids:       {}'.format([i for i in answer_logits if i != pad]))\n",
    "print('  Response Words: {}'.format(\" \".join([target_int_to_letter[i] for i in answer_logits if i != pad])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### OLD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoding - Training\n",
    "Create training logits using [`tf.contrib.seq2seq.simple_decoder_fn_train()`](https://www.tensorflow.org/versions/r1.0/api_docs/python/tf/contrib/seq2seq/simple_decoder_fn_train) and [`tf.contrib.seq2seq.dynamic_rnn_decoder()`](https://www.tensorflow.org/versions/r1.0/api_docs/python/tf/contrib/seq2seq/dynamic_rnn_decoder).  Apply the `output_fn` to the [`tf.contrib.seq2seq.dynamic_rnn_decoder()`](https://www.tensorflow.org/versions/r1.0/api_docs/python/tf/contrib/seq2seq/dynamic_rnn_decoder) outputs."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def decoding_layer_train(encoder_state, dec_cell, dec_embed_input, seq_len, decoding_scope, output_fn, keep_prob):\n",
    "    \n",
    "    with tf.variable_scope('decoding') as decoding_scope:\n",
    "        # tf 1.0:\n",
    "        # train_decoder_fn = tf.contrib.legacy_seq2seq.simple_decoder_fn_train(encoder_state)\n",
    "        \n",
    "        train_decoder_fn = tf.contrib.seq2seq.Decoder() # TODO encoder_state\n",
    "        \n",
    "        dec_cell2 = tf.contrib.rnn.DropoutWrapper(dec_cell, output_keep_prob=keep_prob) # keep probability\n",
    "        \n",
    "        train_pred, _ , _ = tf.contrib.seq2seq.dynamic_rnn_decoder(cell=dec_cell2, decoder_fn=train_decoder_fn, \n",
    "                                                                   inputs=dec_embed_input, sequence_length=seq_len, \n",
    "                                                                   scope=decoding_scope) \n",
    "                                                                # returns (outputs, final_state, final_context_state)\n",
    "        train_logits = output_fn(train_pred)\n",
    "    \n",
    "    return train_logits\n",
    "        \n",
    "    \n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "problem_unittests.test_decoding_layer_train(decoding_layer_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoding - Inference\n",
    "Create inference logits using [`tf.contrib.seq2seq.simple_decoder_fn_inference()`](https://www.tensorflow.org/versions/r1.0/api_docs/python/tf/contrib/seq2seq/simple_decoder_fn_inference) and [`tf.contrib.seq2seq.dynamic_rnn_decoder()`](https://www.tensorflow.org/versions/r1.0/api_docs/python/tf/contrib/seq2seq/dynamic_rnn_decoder). "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def decoding_layer_inference(encoder_state, dec_cell, dec_embeddings, start_of_seq_id, end_of_seq_id, \n",
    "                             max_length, vocab_size, decoding_scope, output_fn, keep_prob):\n",
    "    \n",
    "    with tf.variable_scope('decoding', reuse=False) as decoding_scope:\n",
    "        \n",
    "        inference_decoder_fn = tf.contrib.seq2seq.simple_decoder_fn_inference(encoder_state=encoder_state, output_fn=output_fn, \n",
    "                                                                              embeddings=dec_embeddings, \n",
    "                                                                              start_of_sequence_id=start_of_seq_id, \n",
    "                                                                              end_of_sequence_id=end_of_seq_id, \n",
    "                                                                              maximum_length=max_length, num_decoder_symbols=vocab_size)\n",
    "        \n",
    "        dec_cell2 = tf.contrib.rnn.DropoutWrapper(dec_cell, output_keep_prob=keep_prob) # keep probability\n",
    "        \n",
    "        inference_logits, _ , _ = tf.contrib.seq2seq.dynamic_rnn_decoder(cell=dec_cell2, decoder_fn=inference_decoder_fn, \n",
    "                                                                         scope=decoding_scope) # returns (outputs, final_state, final_context_state)\n",
    "    \n",
    "    return inference_logits\n",
    "        \n",
    "    \n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "problem_unittests.test_decoding_layer_infer(decoding_layer_inference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Build the Decoding Layer\n",
    "Implement `decoding_layer()` to create a Decoder RNN layer.\n",
    "\n",
    "- Create RNN cell for decoding using `rnn_size` and `num_layers`.\n",
    "- Create the output fuction using [`lambda`](https://docs.python.org/3/tutorial/controlflow.html#lambda-expressions) to transform it's input, logits, to class logits.\n",
    "- Use the your `decoding_layer_train(encoder_state, dec_cell, dec_embed_input, sequence_length, decoding_scope, output_fn, keep_prob)` function to get the training logits.\n",
    "- Use your `decoding_layer_infer(encoder_state, dec_cell, dec_embeddings, start_of_sequence_id, end_of_sequence_id, maximum_length, vocab_size, decoding_scope, output_fn, keep_prob)` function to get the inference logits.\n",
    "\n",
    "Note: You'll need to use [tf.variable_scope](https://www.tensorflow.org/api_docs/python/tf/variable_scope) to share variables between training and inference."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def decoding_layer(dec_embed_input, dec_embeddings, encoder_state, vocab_size, sequence_length, rnn_size,\n",
    "                   num_layers, target_vocab_to_int, keep_prob):\n",
    "    \n",
    "    start_of_sequence_id = target_vocab_to_int['<GO>']\n",
    "    end_of_sequence_id = target_vocab_to_int[' <END> ']\n",
    "    \n",
    "    LSTM = tf.contrib.rnn.BasicLSTMCell(rnn_size)\n",
    "    dec_cell = tf.contrib.rnn.MultiRNNCell([LSTM] * num_layers)\n",
    "    # TODO drop-out?\n",
    "    \n",
    "    with tf.variable_scope('decoding') as decoding_scope:\n",
    "        output_fn = lambda x: tf.contrib.layers.fully_connected(x, vocab_size, reuse=None, \n",
    "                                                                scope=decoding_scope, activation_fn=tf.nn.relu) \n",
    "        \n",
    "        train_logits = decoding_layer_train(encoder_state, dec_cell, dec_embed_input, sequence_length, \n",
    "                                            decoding_scope, output_fn, keep_prob)\n",
    "        \n",
    "    with tf.variable_scope('decoding', reuse=True) as decoding_scope:\n",
    "        infer_logits = decoding_layer_inference(encoder_state, dec_cell, dec_embeddings, start_of_sequence_id, \n",
    "                                                end_of_sequence_id, maximum_length, vocab_size, decoding_scope, \n",
    "                                                output_fn, keep_prob)\n",
    "\n",
    "    return train_logits, infer_logits\n",
    "    \n",
    "    \n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "problem_unittests.test_decoding_layer(decoding_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
